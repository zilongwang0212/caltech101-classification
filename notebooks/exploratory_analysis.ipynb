{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f2f4e9",
   "metadata": {},
   "source": [
    "# Caltech-101 Dataset: Exploratory Data Analysis\n",
    "\n",
    "This notebook performs exploratory data analysis on the Caltech-101 dataset including:\n",
    "- Dataset loading and statistics\n",
    "- Class distribution visualization\n",
    "- Sample image visualizations\n",
    "- Data augmentation demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.data.dataset import (\n",
    "    Caltech101Dataset, \n",
    "    get_dataset_statistics,\n",
    "    create_dataloaders,\n",
    "    get_transforms\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854119c1",
   "metadata": {},
   "source": [
    "## 1. Load Dataset and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "DATA_DIR = '../data/101_ObjectCategories'\n",
    "\n",
    "# Load dataset statistics\n",
    "print(\"Computing dataset statistics...\")\n",
    "stats = get_dataset_statistics(DATA_DIR)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total images: {stats['num_images']}\")\n",
    "print(f\"  Number of classes: {stats['num_classes']}\")\n",
    "print(f\"  Mean RGB: {stats['mean']}\")\n",
    "print(f\"  Std RGB: {stats['std']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e40fcb",
   "metadata": {},
   "source": [
    "## 2. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "class_names = stats['class_names']\n",
    "class_counts = list(stats['class_distribution'].values())\n",
    "\n",
    "# Sort by count\n",
    "sorted_indices = np.argsort(class_counts)[::-1]\n",
    "sorted_classes = [class_names[i] for i in sorted_indices]\n",
    "sorted_counts = [class_counts[i] for i in sorted_indices]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.bar(range(len(sorted_classes)), sorted_counts, color='steelblue', alpha=0.7)\n",
    "plt.axhline(y=np.mean(sorted_counts), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {np.mean(sorted_counts):.1f}')\n",
    "plt.xlabel('Class (sorted by frequency)', fontsize=12)\n",
    "plt.ylabel('Number of Images', fontsize=12)\n",
    "plt.title('Caltech-101 Class Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Distribution Summary:\")\n",
    "print(f\"  Mean samples per class: {np.mean(sorted_counts):.2f}\")\n",
    "print(f\"  Median samples per class: {np.median(sorted_counts):.2f}\")\n",
    "print(f\"  Min samples: {np.min(sorted_counts)} ({sorted_classes[-1]})\")\n",
    "print(f\"  Max samples: {np.max(sorted_counts)} ({sorted_classes[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6aa4e9",
   "metadata": {},
   "source": [
    "## 3. Sample Images from Different Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308390b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = Caltech101Dataset(\n",
    "    root_dir=DATA_DIR,\n",
    "    image_size=128,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Select random classes\n",
    "num_classes_to_show = 10\n",
    "num_samples_per_class = 5\n",
    "\n",
    "random_classes = np.random.choice(len(class_names), num_classes_to_show, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(num_classes_to_show, num_samples_per_class, \n",
    "                         figsize=(15, 3 * num_classes_to_show))\n",
    "\n",
    "for i, class_idx in enumerate(random_classes):\n",
    "    # Find all images from this class\n",
    "    class_indices = [idx for idx, label in enumerate(dataset.labels) if label == class_idx]\n",
    "    \n",
    "    # Sample random images\n",
    "    sample_indices = np.random.choice(class_indices, \n",
    "                                     min(num_samples_per_class, len(class_indices)), \n",
    "                                     replace=False)\n",
    "    \n",
    "    for j, idx in enumerate(sample_indices):\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        # Convert to displayable format\n",
    "        img_display = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        axes[i, j].imshow(img_display)\n",
    "        axes[i, j].axis('off')\n",
    "        \n",
    "        if j == 0:\n",
    "            axes[i, j].set_ylabel(class_names[class_idx], fontsize=10, rotation=0, \n",
    "                                 ha='right', va='center')\n",
    "\n",
    "plt.suptitle('Sample Images from Random Classes', fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7c065",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get augmentation transforms\n",
    "train_transform, _ = get_transforms(image_size=128, augmentation=True)\n",
    "\n",
    "# Select one image\n",
    "sample_idx = np.random.choice(len(dataset))\n",
    "image, label = dataset[sample_idx]\n",
    "\n",
    "# Original image (without augmentation)\n",
    "original_image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Apply augmentation multiple times\n",
    "num_augmentations = 8\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "# Show original in center\n",
    "axes[1, 1].imshow(original_image)\n",
    "axes[1, 1].set_title('Original', fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Load PIL image for augmentation\n",
    "from PIL import Image\n",
    "pil_image = Image.open(dataset.image_paths[sample_idx]).convert('RGB')\n",
    "\n",
    "# Apply augmentations\n",
    "positions = [(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2)]\n",
    "for k, (i, j) in enumerate(positions):\n",
    "    augmented = train_transform(pil_image)\n",
    "    \n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    augmented = augmented * std + mean\n",
    "    augmented = torch.clamp(augmented, 0, 1)\n",
    "    \n",
    "    axes[i, j].imshow(augmented.permute(1, 2, 0).numpy())\n",
    "    axes[i, j].set_title(f'Augmentation {k+1}')\n",
    "    axes[i, j].axis('off')\n",
    "\n",
    "plt.suptitle(f'Data Augmentation Examples\\nClass: {class_names[label]}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c566fb2",
   "metadata": {},
   "source": [
    "## 5. Train/Val/Test Split Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders to see split\n",
    "train_loader, val_loader, test_loader, _ = create_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    image_size=128,\n",
    "    batch_size=32,\n",
    "    augmentation=False\n",
    ")\n",
    "\n",
    "# Get split sizes\n",
    "train_size = len(train_loader.dataset)\n",
    "val_size = len(val_loader.dataset)\n",
    "test_size = len(test_loader.dataset)\n",
    "total_size = train_size + val_size + test_size\n",
    "\n",
    "# Visualize split\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "sizes = [train_size, val_size, test_size]\n",
    "labels = ['Train', 'Validation', 'Test']\n",
    "colors = ['#66b3ff', '#ff9999', '#99ff99']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "\n",
    "ax1.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "ax1.set_title('Dataset Split Proportions', fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "ax2.bar(labels, sizes, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Number of Images', fontsize=12)\n",
    "ax2.set_title('Dataset Split Sizes', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(sizes):\n",
    "    ax2.text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"  Train: {train_size} ({100*train_size/total_size:.1f}%)\")\n",
    "print(f\"  Val: {val_size} ({100*val_size/total_size:.1f}%)\")\n",
    "print(f\"  Test: {test_size} ({100*test_size/total_size:.1f}%)\")\n",
    "print(f\"  Total: {total_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5507226",
   "metadata": {},
   "source": [
    "## 6. Image Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6de192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze original image sizes\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Sample random images\n",
    "sample_size = 500\n",
    "sample_indices = random.sample(range(len(dataset)), min(sample_size, len(dataset)))\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "aspects = []\n",
    "\n",
    "for idx in sample_indices:\n",
    "    img_path = dataset.image_paths[idx]\n",
    "    img = Image.open(img_path)\n",
    "    width, height = img.size\n",
    "    widths.append(width)\n",
    "    heights.append(height)\n",
    "    aspects.append(width / height)\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Width distribution\n",
    "axes[0, 0].hist(widths, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Width (pixels)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 0].set_title('Image Width Distribution', fontweight='bold')\n",
    "axes[0, 0].axvline(np.mean(widths), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(widths):.1f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Height distribution\n",
    "axes[0, 1].hist(heights, bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Height (pixels)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 1].set_title('Image Height Distribution', fontweight='bold')\n",
    "axes[0, 1].axvline(np.mean(heights), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(heights):.1f}')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Aspect ratio distribution\n",
    "axes[1, 0].hist(aspects, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Aspect Ratio (width/height)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Aspect Ratio Distribution', fontweight='bold')\n",
    "axes[1, 0].axvline(np.mean(aspects), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(aspects):.2f}')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1, 1].scatter(widths, heights, alpha=0.5, s=10)\n",
    "axes[1, 1].set_xlabel('Width (pixels)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Height (pixels)', fontsize=11)\n",
    "axes[1, 1].set_title('Image Dimensions Scatter', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nImage Size Statistics (from {sample_size} samples):\")\n",
    "print(f\"  Width - Mean: {np.mean(widths):.1f}, Std: {np.std(widths):.1f}\")\n",
    "print(f\"  Height - Mean: {np.mean(heights):.1f}, Std: {np.std(heights):.1f}\")\n",
    "print(f\"  Aspect Ratio - Mean: {np.mean(aspects):.2f}, Std: {np.std(aspects):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f6e46",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This exploratory analysis reveals:\n",
    "- The Caltech-101 dataset contains 101 object categories with varying number of samples per class\n",
    "- Images have different sizes and aspect ratios, requiring resizing for model input\n",
    "- Data augmentation can significantly increase training data diversity\n",
    "- The dataset is split into 70% training, 15% validation, and 15% test sets (stratified)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
