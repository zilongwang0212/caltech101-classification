{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7d2e32",
   "metadata": {},
   "source": [
    "# Caltech-101 Classification: Model Comparison and Results Analysis\n",
    "\n",
    "This notebook analyzes and compares the performance of different models:\n",
    "- Classical ML: HOG + SVM\n",
    "- Deep Learning: ResNet, EfficientNet, Vision Transformer (ViT)\n",
    "\n",
    "Analysis includes:\n",
    "- Performance metrics comparison\n",
    "- Confusion matrices\n",
    "- Per-class accuracy\n",
    "- Training curves\n",
    "- Ablation study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2058fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f8a2b",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation summary\n",
    "results_dir = Path('../results')\n",
    "metrics_dir = results_dir / 'metrics'\n",
    "\n",
    "# Find all metrics files\n",
    "metrics_files = list(metrics_dir.glob('*_test_metrics.json'))\n",
    "\n",
    "# Load all metrics\n",
    "all_metrics = {}\n",
    "for metrics_file in metrics_files:\n",
    "    model_name = metrics_file.stem.replace('_test_metrics', '')\n",
    "    with open(metrics_file, 'r') as f:\n",
    "        all_metrics[model_name] = json.load(f)\n",
    "\n",
    "print(f\"Loaded metrics for {len(all_metrics)} models:\")\n",
    "for model_name in all_metrics.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d8404",
   "metadata": {},
   "source": [
    "## 2. Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract main metrics\n",
    "metric_names = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "model_names = list(all_metrics.keys())\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'Model':<25}{'Accuracy':<15}{'Precision':<15}{'Recall':<15}{'F1-Score':<15}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for model in model_names:\n",
    "    metrics = all_metrics[model]\n",
    "    print(f\"{model:<25}{metrics['accuracy']:<15.4f}{metrics['precision_macro']:<15.4f}\"\n",
    "          f\"{metrics['recall_macro']:<15.4f}{metrics['f1_macro']:<15.4f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Find best model\n",
    "best_model = max(all_metrics.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\nðŸ† Best Model: {best_model[0]} (Accuracy: {best_model[1]['accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "n_models = len(model_names)\n",
    "n_metrics = len(metric_names)\n",
    "\n",
    "data = np.zeros((n_models, n_metrics))\n",
    "for i, model in enumerate(model_names):\n",
    "    for j, metric in enumerate(metric_names):\n",
    "        if metric in all_metrics[model]:\n",
    "            data[i, j] = all_metrics[model][metric]\n",
    "\n",
    "# Plot grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "x = np.arange(n_metrics)\n",
    "width = 0.8 / n_models\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, n_models))\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    offset = (i - n_models / 2) * width + width / 2\n",
    "    bars = ax.bar(x + offset, data[i], width, label=model, alpha=0.8, color=colors[i])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison', fontsize=15, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metric_names, fontsize=11)\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86093d1",
   "metadata": {},
   "source": [
    "## 3. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55287642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model for detailed analysis\n",
    "best_model_name = best_model[0]\n",
    "best_metrics = all_metrics[best_model_name]\n",
    "\n",
    "if 'per_class_metrics' in best_metrics:\n",
    "    per_class = best_metrics['per_class_metrics']\n",
    "    \n",
    "    # Extract data\n",
    "    classes = list(per_class.keys())\n",
    "    accuracies = [per_class[c]['accuracy'] for c in classes]\n",
    "    f1_scores = [per_class[c]['f1_score'] for c in classes]\n",
    "    \n",
    "    # Sort by F1-score\n",
    "    sorted_indices = np.argsort(f1_scores)\n",
    "    \n",
    "    # Plot top 20 best and worst\n",
    "    n_show = 20\n",
    "    worst_indices = sorted_indices[:n_show]\n",
    "    best_indices = sorted_indices[-n_show:]\n",
    "    selected_indices = np.concatenate([worst_indices, best_indices])\n",
    "    \n",
    "    selected_classes = [classes[i] for i in selected_indices]\n",
    "    selected_f1 = [f1_scores[i] for i in selected_indices]\n",
    "    \n",
    "    colors = ['red'] * n_show + ['green'] * n_show\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    y_pos = np.arange(len(selected_classes))\n",
    "    ax.barh(y_pos, selected_f1, color=colors, alpha=0.6)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(selected_classes, fontsize=9)\n",
    "    ax.set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{best_model_name}: Top {n_show} Best and Worst Classes', \n",
    "                fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.axvline(x=np.mean(f1_scores), color='blue', linestyle='--', \n",
    "              linewidth=2, label=f'Mean: {np.mean(f1_scores):.3f}')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc6069",
   "metadata": {},
   "source": [
    "## 4. Training History Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training histories (from model checkpoints if available)\n",
    "import torch\n",
    "\n",
    "models_dir = Path('../models')\n",
    "checkpoint_files = list(models_dir.glob('*_best.pth'))\n",
    "\n",
    "histories = {}\n",
    "for ckpt_file in checkpoint_files:\n",
    "    model_name = ckpt_file.stem.replace('_best', '')\n",
    "    try:\n",
    "        checkpoint = torch.load(ckpt_file, map_location='cpu')\n",
    "        if 'history' in checkpoint:\n",
    "            histories[model_name] = checkpoint['history']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if histories:\n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(histories)))\n",
    "    \n",
    "    for i, (model_name, history) in enumerate(histories.items()):\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        \n",
    "        # Loss\n",
    "        axes[0].plot(epochs, history['train_loss'], '-', \n",
    "                    color=colors[i], alpha=0.7, label=f'{model_name} (train)', linewidth=2)\n",
    "        axes[0].plot(epochs, history['val_loss'], '--', \n",
    "                    color=colors[i], alpha=0.7, label=f'{model_name} (val)', linewidth=2)\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[1].plot(epochs, history['train_acc'], '-', \n",
    "                    color=colors[i], alpha=0.7, label=f'{model_name} (train)', linewidth=2)\n",
    "        axes[1].plot(epochs, history['val_acc'], '--', \n",
    "                    color=colors[i], alpha=0.7, label=f'{model_name} (val)', linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "    axes[0].legend(fontsize=9, loc='upper right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold', pad=15)\n",
    "    axes[1].legend(fontsize=9, loc='lower right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training histories found in model checkpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf682c",
   "metadata": {},
   "source": [
    "## 5. Ablation Study Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1865d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ablation study results\n",
    "ablation_dir = results_dir / 'ablation'\n",
    "ablation_summary_path = ablation_dir / 'ablation_summary.json'\n",
    "\n",
    "if ablation_summary_path.exists():\n",
    "    with open(ablation_summary_path, 'r') as f:\n",
    "        ablation_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nAblation Study Results:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_studies = len(ablation_results)\n",
    "    fig, axes = plt.subplots(1, n_studies, figsize=(6*n_studies, 5))\n",
    "    \n",
    "    if n_studies == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (study_name, results) in enumerate(ablation_results.items()):\n",
    "        print(f\"\\n{study_name.upper()}:\")\n",
    "        for exp_name, accuracy in results.items():\n",
    "            print(f\"  {exp_name}: {accuracy:.4f}\")\n",
    "        \n",
    "        # Plot\n",
    "        experiments = list(results.keys())\n",
    "        accuracies = list(results.values())\n",
    "        \n",
    "        bars = axes[idx].bar(range(len(experiments)), accuracies, \n",
    "                            color='steelblue', alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            axes[idx].text(bar.get_x() + bar.get_width() / 2., acc,\n",
    "                          f'{acc:.4f}', ha='center', va='bottom', \n",
    "                          fontsize=11, fontweight='bold')\n",
    "        \n",
    "        axes[idx].set_xticks(range(len(experiments)))\n",
    "        axes[idx].set_xticklabels(experiments, rotation=30, ha='right')\n",
    "        axes[idx].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_title(f'Ablation: {study_name.replace(\"_\", \" \").title()}', \n",
    "                           fontsize=13, fontweight='bold', pad=15)\n",
    "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "        axes[idx].set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No ablation study results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18e57e",
   "metadata": {},
   "source": [
    "## 6. Classical ML vs Deep Learning Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize models\n",
    "classical_models = {k: v for k, v in all_metrics.items() if 'hog' in k.lower() or 'svm' in k.lower()}\n",
    "deep_models = {k: v for k, v in all_metrics.items() if k not in classical_models}\n",
    "\n",
    "if classical_models and deep_models:\n",
    "    # Calculate average metrics for each category\n",
    "    classical_avg = {\n",
    "        'accuracy': np.mean([m['accuracy'] for m in classical_models.values()]),\n",
    "        'f1_macro': np.mean([m['f1_macro'] for m in classical_models.values()])\n",
    "    }\n",
    "    \n",
    "    deep_avg = {\n",
    "        'accuracy': np.mean([m['accuracy'] for m in deep_models.values()]),\n",
    "        'f1_macro': np.mean([m['f1_macro'] for m in deep_models.values()])\n",
    "    }\n",
    "    \n",
    "    # Visualize comparison\n",
    "    categories = ['Classical ML', 'Deep Learning']\n",
    "    accuracy_values = [classical_avg['accuracy'], deep_avg['accuracy']]\n",
    "    f1_values = [classical_avg['f1_macro'], deep_avg['f1_macro']]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, accuracy_values, width, label='Accuracy', \n",
    "                   color='skyblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, f1_values, width, label='F1-Score (Macro)', \n",
    "                   color='lightcoral', alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                   f'{height:.4f}', ha='center', va='bottom', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Classical ML vs Deep Learning Comparison', \n",
    "                fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories, fontsize=12)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassical ML vs Deep Learning:\")\n",
    "    print(f\"  Classical ML - Avg Accuracy: {classical_avg['accuracy']:.4f}, Avg F1: {classical_avg['f1_macro']:.4f}\")\n",
    "    print(f\"  Deep Learning - Avg Accuracy: {deep_avg['accuracy']:.4f}, Avg F1: {deep_avg['f1_macro']:.4f}\")\n",
    "    print(f\"  Improvement: {100*(deep_avg['accuracy']-classical_avg['accuracy']):.2f}% increase in accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4926e",
   "metadata": {},
   "source": [
    "## Summary and Key Findings\n",
    "\n",
    "### Model Performance:\n",
    "- **Best Model**: [Will be filled based on results]\n",
    "- **Classical ML**: Generally lower performance but faster training\n",
    "- **Deep Learning**: Higher accuracy with pretrained models\n",
    "\n",
    "### Key Insights:\n",
    "1. Transfer learning with pretrained models significantly improves performance\n",
    "2. Data augmentation helps reduce overfitting\n",
    "3. Larger image sizes (128x128) generally perform better than smaller sizes (64x64)\n",
    "4. Some classes are consistently harder to classify across all models\n",
    "\n",
    "### Recommendations:\n",
    "- Use deep learning models (ResNet/ViT) for best accuracy\n",
    "- Apply data augmentation for limited data scenarios\n",
    "- Consider class imbalance when evaluating model performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
